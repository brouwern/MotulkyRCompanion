# Chapter 33: Linear Regression

*Amanda Zielen and Jackie Shane, with Nathan Brouwer*

## Introduction
In Chapter 33 of Intuitive Biostatistics (2nd ED), Motulsky introduces the concept of simple [linear regression](https://en.wikipedia.org/wiki/Linear_regression. The linear regression model uses the slope-intercept form of y= mx +b, where 'm' is the slope and 'b' is the y intercept value for a line. In relation to regression, the slope intercept equation 'y=mx+b' takes different forms; y= b0 + b1x and y= a + bx are two common forms where the first variable is the y intercept and the second variable is the slope of a regression line. In layman's terms, linear regression is often referred to as a trendline, line of best fit, or a regression line.  

Linear regression finds a line that best predicts y from x by calculating the vertical distances (residuals or errors of prediction) of each data point from a line, which minimizes the sum of the squares of those distances (aka, the [least squares](https://en.wikipedia.org/wiki/Least_squares)). The x variable is referred to as the predictor, explanatory, or independent variable. The y variable is referred to as the response, outcome, or dependent variable. This is 'simple' linear regression because there is only one  predictor variable (x). A linear regression line is not 'perfect' as x predicts y but does not *exactly* calculate the observed value of y. An example of a perfect relationship, considered a deterministic relationship, would be the conversion of Celsius to Fahrenheit. (Motulsky 2010, [Pardoe](https://onlinecourses.science.psu.edu/stat501/node/250))

## How to calculate the regression equation (y) = a + bx 

*Further expanded upon in first 'useful link' at the end of the page* 

Slope (b) = (NΣXY - (ΣX)(ΣY)) / (NΣX2 - (ΣX)2)

Intercept (a) = (ΣY - b(ΣX)) / N 

* N  = Number of (x , y) paired data points  
* Xs = First individual value (predictor variables)  
* Ys = Second individual value (response variables)  
* ___ = Sum of the product of all the first and second values  
* ___ = Sum of all the first values  
* ___ = = Sum of all the second values  
* ____ = Sum of the square of all the first values 


## Regression assumptions

In order for linear regression to be applicable, several assumptions must be met (Motulsky 2010): 

1. The relationship between x and y is linear. *This May only be applicable within a defined range of X values*
2. The scatter of data around the line is in a normal (or Gaussian) distribution. 
3. Variation of data points from the linear line is the same everywhere (i.e. all data points have a similar SD from the line).
4. Data points are independent from one another. 
5. X and Y values are independent from one another. *One value should not be incorporated in the other values calculation.*
6: X values are accurately measured.


Motulsky (2010) provides examples emphasizing the importance of the assumptions and the quality of linear regression descriptive statistics. We provide a tutorial on how to replicate Motulsky's figures and tables.


## Motulsky's example with Insulin Sensitivity

Borkman et al. (1993), hypothesized that lipid composition of the skeletal muscle cell membrane affects the insulin sensitivity of the muscle. Their study had 13 males without any known health issues. They infused insulin at a standard rate and quantified how much glucose they needed in order to maintain a constant blood glucose level. Those individuals who are insulin sensitive would require more glucose. Each subject had a small muscle biopsy and the fatty acid composition was measured. Polyunsaturated fatty acids between 20 and 22 carbons (%C20-22) were focused on in the study. Motulsky (2010) regenerated data based upon the Borkman et al. (1993), paper and first used these data in Chapter 32: Correlation.  

<i> Statement: </i> This data set is a good example using linear regression to assess the biological interaction of two variables. A negative Y intercept is biologically impossible which indicates that the data only fits the model for data points within the 'x' value range.

## Load data into R from table 32.1 


The data are on page 244 of the 2nd edition.  Each column is a separate data entry and can be entered in the following format:

Column 1 title <- c (data point 1, data point 2, data point 3, ...)

Then the entire table can be generated by using the data.frame( ) function, using the following format:

Table name <- data.frame(Column 1 title, Column 2 title)

```{r}
Insulin_Sensitivity <- c(250, 220, 145, 115, 230, 200, 330, 400, 370, 260, 270, 530, 375)

Percent_Polyunsaturated <- c(17.9, 18.3, 18.3, 18.4, 18.4, 20.2, 20.3, 21.8, 21.9, 22.1, 23.1, 24.2, 24.4)

Data_Insulin <- data.frame (Insulin_Sensitivity, Percent_Polyunsaturated)

Data_Insulin #to view the data table

```


## Linear regression 

lm() function in R runs a linear regression on the data selected and can be named in an object. In this case, we named it "Ha" referring to the general term "alternative hypothesis". The following format should be used with the lm (x~y, data="") function:

Hypothesis Title <- lm(Column 1 title~ Column 2 title, data= Table name)

The summary() function in R will display useful information from the linear regression. Then important information can be saved in R by assigning it to an object.


Run the linear regression
```{r}
Ha <- lm(Insulin_Sensitivity~Percent_Polyunsaturated, data= Data_Insulin)

summary(Ha) #will display the summary statistics

```

Copy the information from summary()
```{r}
Slope <- "37.21 +/- 9.296" #column 1 row 2 ± column 2 row 2
Yintercept <- "-486.54 +/- 193.71" #column 1 row 1 ± column 2 row 1
Xintercept <- "13.02" # column 1 row 1 / column 1 row 2
Sloperecip <- "0.0268" # calculate 1/column 1 row 2
R2 <- "0.5929" #multiple r squared
Sy.x <- "75.9" #Residual standard error
Fs <- "16.02" #F statistic number
DFnd <- "1, 11" #values calculated with F statistic
Pvalue <-"0.0021" #p value thats listed

```

## Calcualte confidence intervals 

The confint() function in R will provide the 95% confidence intervals for the slope and y intercept, which can be assigned to an object to easily recall later in R.

The paste( ) function in R will allow text and/or functions to be separated by a symbol. 

For example: paste(object 1, object2, sep= "symbol").

The round() function in R will round numbers or calculations to a desired number of places. For example: round (number, places)


Get the confidence intervals
```{r}
CIs<- confint(Ha)
CIs #display confidence intervals
```


Copy the CI information for the teable
```{r}
CIslope <- "16.74 to 57.67"
CIyinter <- "-912.90 to -60.18"
CIxinter <- paste(round((60.17591/16.74752),4), 
                  round((912.90808/57.66797),4), sep= " to ")
```

# Use the lm() and confint() data output to make table 33.1 

Put title names into one vector and results into another vector (i.e:  Title <- c(Row one name, Row two name, Row three name) Results <- c(Row one stat, Row two stat, Row three stat)). Then the two vectors can be put together by using the data.frame ( ) function.

Note: The library, Pander, displays tables nicely and is used in the code below. 

```{r}
Statistic <- c("BEST-FIT VALUES", 
               "Slope", 
               "Y intercept when X=0.0", 
               "X intercept when Y=0.0", 
               "1/slope", "95% CIS", "Slope", 
               "Y intercept when X=0.0", 
               "X intercept when Y=0.0", 
               "GOODNESS OF FIT", 
               "R2", 
               "Sy.x", 
               "IS SLOPE SIGNIFICANTLY NONZERO?", 
               "F", 
               "DFn, DFd",
               "P value", 
               "Deviation from zero?", 
               "DATA", 
               "Number of X values", 
               "Maximum number of Y replicates", 
               "Total number of values", 
               "Number of missing values") 

Result <- c(" ", 
            Slope, 
            Yintercept, 
            Xintercept, 
            Sloperecip, 
            " ", 
            CIslope, 
            CIyinter, 
            CIxinter, 
            " ", 
            R2, 
            Sy.x, 
            " ", 
            Fs, 
            DFnd, 
            Pvalue, "
            Significant **", 
            " ", 
            "13", 
            "1", 
            "13", 
            "0") #" " allow for blank rows in column next to title (capitalized above) 



```


Make the table
```{r}
Table33.1 <- data.frame(Statistic, Result)
library(pander) #the pander package helps display dataframes nicely
pander(Table33.1, justify="left")
```


## Graph linear regression model with and 95% confidence intervals and data  (Figure 33.1 Page 257, 2nd ED)

Use the predict() function to calculate a model based off the linear regression equation and generate lines corresponding to 95% confidence intervals. 

In order to graph, we used R's base plot() function. The plot() function follows this format:

plot(x~y, 
     data= "data frame", 
     xlim = c(left, right), 
     ylim = c(bottom, top), 
     ylab= "y axis title", 
     xlab= "x axis title", 
     pch = #)  
     
NOTE: pch indicates size of data points.

Using the points( ) function in R, lines can be added to the graph (i.e. mean regression line, upper confidence interval line, lower confidence interval line). 

Points (x ~ y, 
        type= "style of line/point", 
        lty = "line type" 
        col= "color of line", 
        data= "data frame")

Multiple lines can be added to the same graph. Any points( ) function under a plot( ) function in R should all be incorporated into the graph.


Put the parts ogether for the graph
```{r}
Modelpredictions <- predict(Ha)

Insulin_SE <- predict(Ha, se.fit= T)$se.fit

CIs <- predict(Ha, interval = "confidence")

Graph.DF <- cbind(Data_Insulin, CIs)
```


Do the plotting
```{r}
plot(Insulin_Sensitivity ~ Percent_Polyunsaturated,
data = Data_Insulin, xlim = c(17, 25), ylim = c(0,600), ylab= expression( "Insulin Sensitivity (mg/"~m^{2}~"/min)"), xlab= "% C20-22 Polyunsaturated Fatty Acids", pch = 16)

points (Modelpredictions ~ Percent_Polyunsaturated, #Add mean line
type = "line", col = "red", data = Data_Insulin)

points(lwr ~ Percent_Polyunsaturated, #Add lower confidence interval
type= "line", lty= "dashed", col = "red", data = Graph.DF)

points(upr ~ Percent_Polyunsaturated, #Add higher confidence interval
type = "line", lty= "dashed", col = "red", data = Graph.DF)
```


## Graph linear regression model with data, 95% confidence intervals, and crossing fitted y= mx+b lines 

Figure 33.3 (Page 259, 2nd ED)

Any line that can be drawn within the confdience interval is plausible.

Making this graph required some trial and error.  In order to generate the lines below, it was important to select an intercept closer to the more negative end of the interval and a slope on the higher end for one line. For the second line, an intercept closer to zero was selected and a smaller slope was selected. This generated the crossed lines.

The plot() function was used to graph insulin sensitivity data points as coded  above. The points() function was used to graph upper and lower condidence interval lines, as coded for above. 

The abline() function was used to graph two lines that fit within the confidence intervals.

The abline function follows this format: 

abline(y intercept, slope, col= "line color")

```{r}
plot(Insulin_Sensitivity ~ Percent_Polyunsaturated,
data = Data_Insulin, 
xlim = c(18, 24), 
ylim = c(0,600), 
ylab= expression( "Insulin Sensitivity (mg/"~m^{2}~"/min)"), xlab= "% C20-22 Polyunsaturated Fatty Acids", pch = 16)

abline(-696, 48, col = "blue")
abline(-100, 19, col = "blue")

points(lwr ~ Percent_Polyunsaturated, #lower confidence interval
type= "line", lty= "dashed", col = "red",
data = Graph.DF)
points(upr ~ Percent_Polyunsaturated, #higher confidence interval 
type = "line", lty= "dashed", col = "red",
data = Graph.DF)
```


## Motulsky's example with random data

Motulsky generated random data sampled from a Gaussian distribution where the mean=120 and the SD=10, simulating blood pressures of individuals "before" and "after" an event. 

In order for statistical tests with linear regression to be valid, both the x and y variables need to be independent of one another. When linear regression is done using data such "before" vs. "after"  the R^2^ and p-values are not valid.   

In this example, Motulsky demonstrates how randomly generated data can produce highly correalted data (High R^2^) and a "significant" linear trend.  He illustrates this with hypothetical "Change from baseline (Before-After)" vs. "Starting BP (Before)".

### Generate random data 

Use the function rnorm( ) to generate normally distributed data defined by a given mean, SD and sample side.. 

Note: The function set.seed() fixes the resutls of ther andom number generate so the results are reproducible.
```{r}

set.seed(2356) #set "random factor" for reproducible randomized data
Fake_n <- 25 #set sample size
Fake_mean <- 120 #set mean
Fake_SD <- 10 #set SD

Before <- rnorm(n = Fake_n, #generate "before" data using rnorm
      mean = Fake_mean,
      sd = Fake_SD)
 
After <- rnorm(n = Fake_n,
      mean = Fake_mean,
      sd = Fake_SD)

#Calculate statistics from generated data
Mean.before <- mean(Before)
Mean.after <- mean(After)
Means <- c(Mean.before, Mean.after)

When <- c(rep("Before", Fake_n),rep("After", Fake_n)) 
Fakedata <- data.frame(bp.mmHg = c(Before, After), when = When)

FakeLR_independent <- lm(After ~ Before)
summary(FakeLR_independent)

FakeLR_dep <- lm(After - Before ~ Before)
summary(FakeLR_dep)

```


##. Make a figure similar to Figure 33.5 </b> (Page 264, 2nd ED)

The library, beeswarm, provides a nice format to make a dot blot. 
Plot ( ) was used for the other two graphs. 

The function par ( ) allows for multiple graphs to be formatted into one figure. 

The function text ( ) inserts text on the graph at the (x,y) points defined in the following format:
text (x, y, "text", cex= "font size", col= "color of text")

Correlation (R^2^) and linear regression are related and their pvalue statistics are identical. This is because each null hypothesis states "there is no [linear or otherwise] relationship between variables." Which is why y=mx+b equations for regression of data often have R^2^ values reported. R^2^ values range from 0 (no linear relationship) to 1 (perfect linear relationship). While comparing this 'random data' R^2^ was reported to show strong correlation between "Change in pressure" vs. "Baseline" because the x values were used in calculating y values. 

```{r}

library(beeswarm)

par(mfrow = c(2,2))
beeswarm(bp.mmHg ~ When, method= "hex", data = Fakedata, xlab= " ", ylab=" ")
points(Means, pch= "---", cex= 4, col="forest green")

plot(After ~ Before)
abline(FakeLR_independent)
text(132, 126, "y= -0.147x + 136.22", cex=0.75, col="black")
text(132, 123, expression(""~R^{2}~"=0.045, p = 0.31"), cex=0.75, col="forest green")

plot(After - Before ~ Before, xlab= "Baseline", ylab= "Change in pressure")
abline(FakeLR_dep)
text(130, 16, "y= 1.14x + 136.22", cex=0.75, col="black")
text(130, 9, expression(""~R^{2}~"=0.741, p <0.001"), cex=0.75, col="forest green")


```


## Motulsky's example with Anscombe's quartet

Statistician Francis Anscombe generated four data sets with the same descriptive statistics (slope, y intercept, R^2^ value) but with very different graphical distributions (1973). 

Motulsky utilized Anscombe's example to emphasize the importance of graphing data before calculating statistics, to ensure that the data fit the statistical test parameters. In essence, these four data sets look very different, but provide the same descriptive statistics. Only the top left graph appears to be in a linear distribution.

###  Load data into R from wikipedia <

html: https://en.wikipedia.org/wiki/Anscombe%27s_quartet

```{r}
x123 <- c(10,8,13,9,11,14,6,4,12,7,5) #x1, x2, and x3 data points are all the same.
x4 <- c(8,8,8,8,8,8,8,19,8,8,8)
y1 <- c(8.04,6.95,7.58,8.81,8.33,9.96,7.24,4.26,10.84,4.82,5.68)
y2 <- c(9.14,8.14,8.74,8.77,9.26,8.1,6.13,3.1,9.13,7.26,4.74)
y3 <- c(7.46,6.77,12.74,7.11,7.81,8.84,6.08,5.39,8.15,6.42,5.73)
y4 <- c(6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.5, 5.56, 7.91, 6.89)
```

### Generate Figure 33.6 with linear regression lines and Anscombe's data points for all four X-Y data sets </b> (Page 266, 2nd ED)

```{r}
#Examine summary statistics of each linear regression
summary(lm(y1~x123))
summary(lm(y2~x123))
summary(lm(y3~x123))
summary(lm(y4~x4))

#Use par to plot a 2 by 2 grid of graphs
par(mfrow = c(2,2))

plot(y1~x123,
xlim= c(0,20), ylim= c(0,18), xlab= "x1", ylab = "y1", pch=15, cex=1)
abline(lm(y1~x123))
text(13, 17, "y= 0.50x + 3.0", cex=0.75, col="black")
text(13, 15, expression(""~R^{2}~"=0.66, p=0.002"), cex=0.75, col="purple")

plot(y2~x123,
xlim= c(0,20), ylim= c(0,18), xlab= "x2", ylab = "y2", pch=15, cex=1)
abline(lm(y2~x123))
text(13, 17, "y= 0.50x + 3.0", cex=0.75, col="black")
text(13, 15, expression(""~R^{2}~"=0.66, p=0.002"), cex=0.75, col="purple")

plot(y3~x123,
xlim= c(0,20), ylim= c(0,18), xlab= "x3", ylab = "y3", pch=15, cex=1)
abline(lm(y3~x123))
text(13, 17, "y= 0.50x + 3.0", cex=0.75, col="black")
text(13, 15, expression(""~R^{2}~"=0.66, p=0.002"), cex=0.75, col="purple")

plot(y4~x4,
xlim= c(0,20), ylim= c(0,18), xlab= "x4", ylab = "43", pch=15, cex=1)
abline(lm(y4~x4))
text(13, 17, "y= 0.50x + 3.0", cex=0.75, col="black")
text(13, 15, expression(""~R^{2}~"=0.66, p=0.002"), cex=0.75, col="purple")

```

## Conclusion
Linear regression is useful for data that when graphed appears to correlate to a line. Motulsky provided examples to demonstrate how violating assumptions of the linear regression model affect statistical output, and we showed how to replicate these examples in R. 

Though insulin sensitivity and % C20-22 fatty acid muscle composition appear to be significantly correlated (p=0.002) to one another, the y intercept is not biologically possible, which emphasizes the linear model is only applicable to x values within the range of data observed. 

The generation of random data and comparing linear regression of two models, the first with X ~ Y and the second Y-X ~ Y, enforces the principle that x and y variables need to be measured or calculated, independently. If one variable is used to calculate the other, a false linear relationship can be supported (i.e. correlation between randomly generated data).  

The Anscombe's quartet example emphasizes the importance of graphing data to check that a linear model is applicable. 

In order for linear regression to be relevant, collect accurate and independent predictor and response variables, check that the data adhere to the six assumptions listed above, and make sure to graphically represent your data!


## References
Anscombe, F. J. (1973). "Graphs in Statistical Analysis". American Statistician. 27: 1973.

"Anscombe's quartet" Wikipedia: The Free Encyclopedia. <https://en.wikipedia.org/wiki/Anscombe%27s_quartet>

Borkman, Mark, et al. (1993). "The relation between insulin sensitivity and the fatty-acid composition of skeletal-muscle phospholipids." <i> New England Journal of Medicine. </i> 328 (4): 238-244.

Motulsky, Harvey. (2010). <u> Intuitive biostatistics a nonmathmatical guide to statistical thinking.</u>  <i> 2nd edition. </i>

Pardoe, Iain. "STAT 501: Lesson 1: Simple Linear Regression." PennState Eberly College of Science. https://onlinecourses.science.psu.edu/stat501/node/250   

## Useful links
This webpage provides a simple example of how to calculate linear regression by hand. 
https://www.easycalculation.com/statistics/learn-regression.php 

Quickcalcs by Graphpad, provides a webbased format to input x and y variables and calculate the data's linear regression. Note: This quickcalcs site can calculate other statistics. 
http://www.graphpad.com/quickcalcs/linear1/

This webpage explains linear regression and provides examples to calculate linear regression in excel and on a TI-83 calculator. 
http://www.statisticshowto.com/how-to-find-a-linear-regression-equation/

See here for something similar to Anscombe's quartert, but much crazier
https://www.autodeskresearch.com/publications/samestats
